{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification with the Naive Bayes algorithm #\n",
    "\n",
    "**Machine Learning** (ML) is a sub-field of Artificial Intelligence (AI) that automates analytical model building. As the amount of data grows, ML is becoming the defacto standard within all research fields. So what does it mean that a machine *learns*? Given enough data, we can offload programming tasks to the algorithm, that is, the algorithm learns structure from the data:\n",
    "\n",
    "Tom Mitchell's **A well-posed learning problem**: \"A computer program is said to learn from experience $E$ with respect to some task $T$ and some performance measure $P$, if its performance on $T$, as measured by $P$, improves with experience $E$\"\n",
    "\n",
    "**Classification** is one is a primary task of supervised learning. Given labeled data, a classification algorithm will output a solution that categorizes new examples (i.e., associate labels with subsets of the data). While unsupervised learning searches for groups within the data, classification learns to map a data set onto a categorical class values or labels (i.e., function approximation).\n",
    "\n",
    "The **Naive Bayes** (NB) algorithm is a generative algorithm that is very popular for text classification. \n",
    "The probability of a document $d$ being in class $c$, $P(c \\mid d)$ is computed as:\n",
    "\n",
    "\n",
    "$$ P(c \\mid d) \\propto P(c) \\prod_{i = 1}^{m}P(t_i \\mid c) $$\n",
    "and the class of a document $d$ is then computed as:\n",
    "\n",
    "$$c_{MAP} = arg~max_{c \\in \\{c_1, c_2 \\}} P(c \\mid d)$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research problem ##\n",
    "\n",
    "The medieval writer Saxo Grammatricus (c. 1160 - post 1208) represents the beginning of the modern day historian in Scandinavia. Saxo's history of the Danes *Gesta Danorum* (\"Deeds of the Danes\") is the single most important written source to Danish history in the 12<sup>th</sup> century. *Gesta Danorum*  is tendentious, contains elements of fiction, and its compositions has been an academic subject of debate for more than a century. The more recent debate treat the bipartite composition \\emph{Gesta Danorum} and centers on two related issues: 1) is the transition between the old mythical and new historical part located in book eight, nine, or ten; and 2) is this transition gradual (continuous) or sudden (point-like)? In this tutorial we will ask \"is book nine more similar to the early books (1-8) or the late books (10-16)\". This is an example uses simple vector space techniques from author chronometry to represent the most salient stylistic and semantic leaxical features of *Gesta Danorum*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data ###\n",
    "We start by importing *Gesta Danorum* sampled in slices of 250 words (see `make_data.py` for more detail)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "data_path = os.path.join(\"..\",\"data\",\"saxo_books\",\"saxo_class.csv\")\n",
    "\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "print(data.head())\n",
    "\n",
    "print(set(data[\"book_class\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data set in training data and to-be determined data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = data[\"book_class\"] == \"uncertain\"\n",
    "\n",
    "data_uncertain = data[idx]\n",
    "data = data[~idx]\n",
    "\n",
    "print(set(data[\"book_class\"]))\n",
    "print(set(data_uncertain[\"book_class\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CLASS DIST AND BIAS\n",
    "def printdist(df):\n",
    "    for label in set(df['book_class']):\n",
    "        print(\"number of \" + label + \": {}\".format(sum(df['book_class'] == label)))\n",
    "\n",
    "printdist(data)\n",
    "printdist(data_uncertain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split training data in a training set and a test set to avoid overfitting ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT DATA SET\n",
    "ratio = .8\n",
    "mask = np.random.rand(len(data)) <= ratio\n",
    "train = data[mask]\n",
    "test = data[~mask]\n",
    "\n",
    "## training set\n",
    "X_train = train[\"content\"].values\n",
    "y_train = train[\"book_class\"].values\n",
    "## test set\n",
    "X_test = test[\"content\"].values\n",
    "y_test = test[\"book_class\"].values\n",
    "\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build document vector space ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range = (1,2), stop_words = None,\n",
    "    lowercase = True, max_df = .9, min_df = .01, max_features = 500)\n",
    "\n",
    "feature_train = vectorizer.fit_transform(X_train)\n",
    "feature_test =  vectorizer.transform(X_test)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "print(len(feature_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Naive Bayes classifier ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN CLASSIFIER\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(feature_train, y_train)\n",
    "\n",
    "# EVALUATION\n",
    "pred = nb_classifier.predict(feature_test)\n",
    "# horizontal: P(y|X); vertical: y\n",
    "confmat = metrics.confusion_matrix(y_test, pred)\n",
    "print(confmat)\n",
    "# obeserved accuracy\n",
    "print(\"Accurracy: {}\".format(round(metrics.accuracy_score(y_test, pred),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify the to-be-determined data using \"\"###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_uncertain = vectorizer.transform(data_uncertain[\"content\"])\n",
    "uncertain_class = nb_classifier.predict(feature_uncertain)\n",
    "decision = Counter(uncertain_class)\n",
    "\n",
    "print(\"books 9 has {} votes for early style and {} for late style\".format(decision[\"early\"],decision[\"late\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
